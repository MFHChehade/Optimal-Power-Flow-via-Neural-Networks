{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oF96lcR9VKG0"
   },
   "source": [
    "## Import some libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 12,
     "status": "ok",
     "timestamp": 1669669130787,
     "user": {
      "displayName": "Mohamad Fares El Hajj Chehade",
      "userId": "11973934474938501082"
     },
     "user_tz": -120
    },
    "id": "XYyx-BYgSm7j"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Specify the Power Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 30 # number of nodes \n",
    "\n",
    "nodes = range(n) # enumerates all node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "network_name = \"ieee\" + str(n)\n",
    "filename = network_name + \".xlsx\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NdFgjCfGVVxL"
   },
   "source": [
    "## Import the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 300
    },
    "executionInfo": {
     "elapsed": 10,
     "status": "ok",
     "timestamp": 1669668808713,
     "user": {
      "displayName": "Mohamad Fares El Hajj Chehade",
      "userId": "11973934474938501082"
     },
     "user_tz": -120
    },
    "id": "J4oMAeIeVOI9",
    "outputId": "e6e1d996-64de-4642-944e-33c9ce904b03"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PD1</th>\n",
       "      <th>PD2</th>\n",
       "      <th>PD3</th>\n",
       "      <th>PD4</th>\n",
       "      <th>PD5</th>\n",
       "      <th>PD6</th>\n",
       "      <th>PD7</th>\n",
       "      <th>PD8</th>\n",
       "      <th>PD9</th>\n",
       "      <th>PD10</th>\n",
       "      <th>...</th>\n",
       "      <th>PD17</th>\n",
       "      <th>PD18</th>\n",
       "      <th>PD19</th>\n",
       "      <th>PD20</th>\n",
       "      <th>PG1</th>\n",
       "      <th>PG2</th>\n",
       "      <th>PG3</th>\n",
       "      <th>PG4</th>\n",
       "      <th>PG5</th>\n",
       "      <th>PG6</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>19.788746</td>\n",
       "      <td>2.487347</td>\n",
       "      <td>6.904495</td>\n",
       "      <td>20.845791</td>\n",
       "      <td>30.129899</td>\n",
       "      <td>5.332207</td>\n",
       "      <td>11.912653</td>\n",
       "      <td>6.593758</td>\n",
       "      <td>8.564801</td>\n",
       "      <td>3.254906</td>\n",
       "      <td>...</td>\n",
       "      <td>8.582361</td>\n",
       "      <td>3.727720</td>\n",
       "      <td>2.200066</td>\n",
       "      <td>9.822323</td>\n",
       "      <td>44.394362</td>\n",
       "      <td>57.879270</td>\n",
       "      <td>22.206196</td>\n",
       "      <td>31.521251</td>\n",
       "      <td>15.515489</td>\n",
       "      <td>15.515489</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20.282507</td>\n",
       "      <td>2.347650</td>\n",
       "      <td>8.103697</td>\n",
       "      <td>24.183342</td>\n",
       "      <td>27.362827</td>\n",
       "      <td>5.683139</td>\n",
       "      <td>11.260202</td>\n",
       "      <td>6.096831</td>\n",
       "      <td>8.457250</td>\n",
       "      <td>3.589581</td>\n",
       "      <td>...</td>\n",
       "      <td>8.477993</td>\n",
       "      <td>3.288683</td>\n",
       "      <td>2.395050</td>\n",
       "      <td>10.259726</td>\n",
       "      <td>43.901203</td>\n",
       "      <td>57.315660</td>\n",
       "      <td>22.048385</td>\n",
       "      <td>30.338615</td>\n",
       "      <td>15.120962</td>\n",
       "      <td>15.120962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>23.660076</td>\n",
       "      <td>2.601759</td>\n",
       "      <td>6.920069</td>\n",
       "      <td>23.884633</td>\n",
       "      <td>28.614717</td>\n",
       "      <td>5.710489</td>\n",
       "      <td>11.307231</td>\n",
       "      <td>6.748994</td>\n",
       "      <td>8.065100</td>\n",
       "      <td>3.838137</td>\n",
       "      <td>...</td>\n",
       "      <td>8.139950</td>\n",
       "      <td>3.239610</td>\n",
       "      <td>2.639559</td>\n",
       "      <td>9.902777</td>\n",
       "      <td>44.974087</td>\n",
       "      <td>58.541814</td>\n",
       "      <td>22.391708</td>\n",
       "      <td>32.911480</td>\n",
       "      <td>15.979270</td>\n",
       "      <td>15.979270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>19.671488</td>\n",
       "      <td>2.429376</td>\n",
       "      <td>8.180437</td>\n",
       "      <td>23.571439</td>\n",
       "      <td>28.142600</td>\n",
       "      <td>5.647943</td>\n",
       "      <td>11.112026</td>\n",
       "      <td>6.797231</td>\n",
       "      <td>7.636504</td>\n",
       "      <td>3.748866</td>\n",
       "      <td>...</td>\n",
       "      <td>8.855743</td>\n",
       "      <td>3.308331</td>\n",
       "      <td>2.344617</td>\n",
       "      <td>10.775931</td>\n",
       "      <td>44.246910</td>\n",
       "      <td>57.710754</td>\n",
       "      <td>22.159011</td>\n",
       "      <td>31.167648</td>\n",
       "      <td>15.397528</td>\n",
       "      <td>15.397528</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20.622839</td>\n",
       "      <td>2.299412</td>\n",
       "      <td>7.777978</td>\n",
       "      <td>21.729681</td>\n",
       "      <td>31.946258</td>\n",
       "      <td>6.359890</td>\n",
       "      <td>11.715757</td>\n",
       "      <td>6.006407</td>\n",
       "      <td>8.337874</td>\n",
       "      <td>3.225438</td>\n",
       "      <td>...</td>\n",
       "      <td>8.569951</td>\n",
       "      <td>3.368903</td>\n",
       "      <td>2.237513</td>\n",
       "      <td>9.918984</td>\n",
       "      <td>44.886903</td>\n",
       "      <td>58.442175</td>\n",
       "      <td>22.363809</td>\n",
       "      <td>32.702406</td>\n",
       "      <td>15.909523</td>\n",
       "      <td>15.909523</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         PD1       PD2       PD3        PD4        PD5       PD6        PD7  \\\n",
       "0  19.788746  2.487347  6.904495  20.845791  30.129899  5.332207  11.912653   \n",
       "1  20.282507  2.347650  8.103697  24.183342  27.362827  5.683139  11.260202   \n",
       "2  23.660076  2.601759  6.920069  23.884633  28.614717  5.710489  11.307231   \n",
       "3  19.671488  2.429376  8.180437  23.571439  28.142600  5.647943  11.112026   \n",
       "4  20.622839  2.299412  7.777978  21.729681  31.946258  6.359890  11.715757   \n",
       "\n",
       "        PD8       PD9      PD10  ...      PD17      PD18      PD19       PD20  \\\n",
       "0  6.593758  8.564801  3.254906  ...  8.582361  3.727720  2.200066   9.822323   \n",
       "1  6.096831  8.457250  3.589581  ...  8.477993  3.288683  2.395050  10.259726   \n",
       "2  6.748994  8.065100  3.838137  ...  8.139950  3.239610  2.639559   9.902777   \n",
       "3  6.797231  7.636504  3.748866  ...  8.855743  3.308331  2.344617  10.775931   \n",
       "4  6.006407  8.337874  3.225438  ...  8.569951  3.368903  2.237513   9.918984   \n",
       "\n",
       "         PG1        PG2        PG3        PG4        PG5        PG6  \n",
       "0  44.394362  57.879270  22.206196  31.521251  15.515489  15.515489  \n",
       "1  43.901203  57.315660  22.048385  30.338615  15.120962  15.120962  \n",
       "2  44.974087  58.541814  22.391708  32.911480  15.979270  15.979270  \n",
       "3  44.246910  57.710754  22.159011  31.167648  15.397528  15.397528  \n",
       "4  44.886903  58.442175  22.363809  32.702406  15.909523  15.909523  \n",
       "\n",
       "[5 rows x 26 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = pd.read_excel(filename,sheet_name = \"dataset\")\n",
    "\n",
    "dataset.head() # Returns the first 5 rows in the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_type = pd.read_excel(filename, sheet_name = \"n\") # if type is load or generator \n",
    "\n",
    "n_loads = node_type[\"n_loads\"][0] # get the number of generator nodes\n",
    "n_gens = node_type[\"n_gens\"][0] # get the number of load nodes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1669668808715,
     "user": {
      "displayName": "Mohamad Fares El Hajj Chehade",
      "userId": "11973934474938501082"
     },
     "user_tz": -120
    },
    "id": "aeYIvEjpe1Wu"
   },
   "outputs": [],
   "source": [
    "# Changing pandas dataframe to numpy arrays:\n",
    "\n",
    "X = dataset.iloc[:,:n_loads].values\n",
    "y = dataset.iloc[:,n_loads:n_loads+n_gens].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalize the target "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bus</th>\n",
       "      <th>Pg</th>\n",
       "      <th>Qg</th>\n",
       "      <th>Qmax</th>\n",
       "      <th>Qmin</th>\n",
       "      <th>Vg</th>\n",
       "      <th>mBase</th>\n",
       "      <th>status</th>\n",
       "      <th>Pmax</th>\n",
       "      <th>Pmin</th>\n",
       "      <th>...</th>\n",
       "      <th>Pc2</th>\n",
       "      <th>Qc1min</th>\n",
       "      <th>Qc1max</th>\n",
       "      <th>Qc2min</th>\n",
       "      <th>Qc2max</th>\n",
       "      <th>ramp_agc</th>\n",
       "      <th>ramp_10</th>\n",
       "      <th>ramp_30</th>\n",
       "      <th>ramp_q</th>\n",
       "      <th>apf</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>23.54</td>\n",
       "      <td>0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>-20</td>\n",
       "      <td>1</td>\n",
       "      <td>100</td>\n",
       "      <td>1</td>\n",
       "      <td>80</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>60.97</td>\n",
       "      <td>0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>-20</td>\n",
       "      <td>1</td>\n",
       "      <td>100</td>\n",
       "      <td>1</td>\n",
       "      <td>80</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>21</td>\n",
       "      <td>21.59</td>\n",
       "      <td>0</td>\n",
       "      <td>62.5</td>\n",
       "      <td>-15</td>\n",
       "      <td>1</td>\n",
       "      <td>100</td>\n",
       "      <td>1</td>\n",
       "      <td>50</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>26</td>\n",
       "      <td>26.91</td>\n",
       "      <td>0</td>\n",
       "      <td>48.7</td>\n",
       "      <td>-15</td>\n",
       "      <td>1</td>\n",
       "      <td>100</td>\n",
       "      <td>1</td>\n",
       "      <td>55</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>22</td>\n",
       "      <td>19.20</td>\n",
       "      <td>0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>-10</td>\n",
       "      <td>1</td>\n",
       "      <td>100</td>\n",
       "      <td>1</td>\n",
       "      <td>30</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   bus     Pg  Qg   Qmax  Qmin  Vg  mBase  status  Pmax  Pmin  ...  Pc2  \\\n",
       "0    0  23.54   0  150.0   -20   1    100       1    80     0  ...    0   \n",
       "1    1  60.97   0   60.0   -20   1    100       1    80     0  ...    0   \n",
       "2   21  21.59   0   62.5   -15   1    100       1    50     0  ...    0   \n",
       "3   26  26.91   0   48.7   -15   1    100       1    55     0  ...    0   \n",
       "4   22  19.20   0   40.0   -10   1    100       1    30     0  ...    0   \n",
       "\n",
       "   Qc1min  Qc1max  Qc2min  Qc2max  ramp_agc  ramp_10  ramp_30  ramp_q  apf  \n",
       "0       0       0       0       0         0        0        0       0    0  \n",
       "1       0       0       0       0         0        0        0       0    0  \n",
       "2       0       0       0       0         0        0        0       0    0  \n",
       "3       0       0       0       0         0        0        0       0    0  \n",
       "4       0       0       0       0         0        0        0       0    0  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load generator data\n",
    "\n",
    "gen = pd.read_excel(filename,sheet_name=\"Gen\")\n",
    "\n",
    "gen.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store the maximum and minimum power vectors \n",
    "\n",
    "P_G_max =  gen[\"Pmax\"].values\n",
    "\n",
    "P_G_min = gen[\"Pmin\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1669668808716,
     "user": {
      "displayName": "Mohamad Fares El Hajj Chehade",
      "userId": "11973934474938501082"
     },
     "user_tz": -120
    },
    "id": "A9lK2dbMc5KO",
    "outputId": "f1c8e997-6779-4082-b64f-43501d8afed8"
   },
   "outputs": [],
   "source": [
    "# normalize the target\n",
    "\n",
    "N = len(y) # number of samples\n",
    "\n",
    "for i in range(N):\n",
    "    for j in range(n_gens):\n",
    "        y[i][j] = ( y[i][j] - P_G_min[j] ) / (P_G_max[j] - P_G_min[j])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split the data & Normalize the input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "executionInfo": {
     "elapsed": 486,
     "status": "ok",
     "timestamp": 1669668809193,
     "user": {
      "displayName": "Mohamad Fares El Hajj Chehade",
      "userId": "11973934474938501082"
     },
     "user_tz": -120
    },
    "id": "4X2Kz9xOfvfd"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size = 0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Feature Scaling of datasets  \n",
    "\n",
    "from sklearn.preprocessing import StandardScaler  \n",
    "\n",
    "st_x= StandardScaler()  \n",
    "\n",
    "X_train= st_x.fit_transform(X_train)  \n",
    "\n",
    "X_test= st_x.transform(X_test) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i9_T1T5tf0Wb"
   },
   "source": [
    "# Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "executionInfo": {
     "elapsed": 14,
     "status": "ok",
     "timestamp": 1669668809194,
     "user": {
      "displayName": "Mohamad Fares El Hajj Chehade",
      "userId": "11973934474938501082"
     },
     "user_tz": -120
    },
    "id": "W3Q3U8-Tf1mt"
   },
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kYpTwR6Cf4ix"
   },
   "source": [
    "### Network Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "executionInfo": {
     "elapsed": 14,
     "status": "ok",
     "timestamp": 1669668809195,
     "user": {
      "displayName": "Mohamad Fares El Hajj Chehade",
      "userId": "11973934474938501082"
     },
     "user_tz": -120
    },
    "id": "JdlNoigZf6UK"
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "initializer = keras.initializers.HeUniform(seed=1)\n",
    "\n",
    "layer1 = Dense(32, input_dim=n_loads, activation='relu', kernel_initializer = initializer)\n",
    "layer2 = Dense(16, activation='relu',kernel_initializer = initializer)\n",
    "layer3 = Dense(8, activation='relu',kernel_initializer = initializer)\n",
    "layer4 = Dense(n_gens, activation='linear',kernel_initializer = initializer)\n",
    "\n",
    "model.add(layer1)\n",
    "model.add(layer2)\n",
    "model.add(layer3)\n",
    "model.add(layer4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4xbAVyAMW6FL"
   },
   "source": [
    "## Draw the network architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 533
    },
    "executionInfo": {
     "elapsed": 15,
     "status": "ok",
     "timestamp": 1669668809196,
     "user": {
      "displayName": "Mohamad Fares El Hajj Chehade",
      "userId": "11973934474938501082"
     },
     "user_tz": -120
    },
    "id": "xFPMXpRsW7T1",
    "outputId": "ed652683-24ab-47ad-f5dd-825717f8369d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) for plot_model to work.\n"
     ]
    }
   ],
   "source": [
    "from keras.utils.vis_utils import plot_model\n",
    "plot_model(model, to_file='model_plot.png', show_shapes=True, show_layer_names=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5aL-uLoRgLIw"
   },
   "source": [
    "### Loss Function and Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras.backend as K\n",
    "\n",
    "def custom_mse(y_true, y_pred):\n",
    " \n",
    "    # calculating squared difference between target and predicted values \n",
    "    loss1 = K.square(y_pred - y_true)  # (batch_size, 2)\n",
    "                \n",
    "    # summing both loss values along batch dimension \n",
    "    loss1 = K.mean(loss1, axis=1)        # (batch_size,)\n",
    "    \n",
    "    # generator limits \n",
    "    \n",
    "    loss2 = K.maximum(y_pred - 1, 0)\n",
    "    loss2 = K.mean(loss2, axis = 1)\n",
    "    \n",
    "    loss3 = K.maximum(- y_pred , 0)\n",
    "    loss3 = K.mean(loss3, axis = 1)\n",
    "    \n",
    "    w1 = 1\n",
    "    w2 = 1\n",
    "    \n",
    "    loss = w1 * loss1 + w2 * (loss2 + loss3)\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "executionInfo": {
     "elapsed": 14,
     "status": "ok",
     "timestamp": 1669668809197,
     "user": {
      "displayName": "Mohamad Fares El Hajj Chehade",
      "userId": "11973934474938501082"
     },
     "user_tz": -120
    },
    "id": "CKIJr6uFf7aE"
   },
   "outputs": [],
   "source": [
    "sgd = keras.optimizers.SGD(learning_rate=1e-3, momentum = 0.5) \n",
    "\n",
    "adam = keras.optimizers.Adam(learning_rate = 1e-3)\n",
    "\n",
    "#model.compile(loss=custom_mse, optimizer= adam, metrics=['mae'])\n",
    "model.compile(loss='mse', optimizer=adam, metrics=['mse'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nAtG0Ttqgi5H"
   },
   "source": [
    "### Train the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 319414,
     "status": "ok",
     "timestamp": 1669669128598,
     "user": {
      "displayName": "Mohamad Fares El Hajj Chehade",
      "userId": "11973934474938501082"
     },
     "user_tz": -120
    },
    "id": "eovEX_1rgjtM",
    "outputId": "221b3c8f-ff8d-47c2-c997-67cb632e0d53"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "88/88 [==============================] - 3s 18ms/step - loss: 0.2382 - mse: 0.2382 - val_loss: 0.0892 - val_mse: 0.0892\n",
      "Epoch 2/300\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.0448 - mse: 0.0448 - val_loss: 0.0224 - val_mse: 0.0224\n",
      "Epoch 3/300\n",
      "88/88 [==============================] - 0s 4ms/step - loss: 0.0154 - mse: 0.0154 - val_loss: 0.0115 - val_mse: 0.0115\n",
      "Epoch 4/300\n",
      "88/88 [==============================] - 0s 4ms/step - loss: 0.0088 - mse: 0.0088 - val_loss: 0.0075 - val_mse: 0.0075\n",
      "Epoch 5/300\n",
      "88/88 [==============================] - 1s 8ms/step - loss: 0.0060 - mse: 0.0060 - val_loss: 0.0055 - val_mse: 0.0055\n",
      "Epoch 6/300\n",
      "88/88 [==============================] - 1s 8ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0043 - val_mse: 0.0043\n",
      "Epoch 7/300\n",
      "88/88 [==============================] - 1s 8ms/step - loss: 0.0036 - mse: 0.0036 - val_loss: 0.0034 - val_mse: 0.0034\n",
      "Epoch 8/300\n",
      "88/88 [==============================] - 1s 6ms/step - loss: 0.0029 - mse: 0.0029 - val_loss: 0.0029 - val_mse: 0.0029\n",
      "Epoch 9/300\n",
      "88/88 [==============================] - 1s 8ms/step - loss: 0.0024 - mse: 0.0024 - val_loss: 0.0024 - val_mse: 0.0024\n",
      "Epoch 10/300\n",
      "88/88 [==============================] - 1s 8ms/step - loss: 0.0021 - mse: 0.0021 - val_loss: 0.0021 - val_mse: 0.0021\n",
      "Epoch 11/300\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.0018 - mse: 0.0018 - val_loss: 0.0018 - val_mse: 0.0018\n",
      "Epoch 12/300\n",
      "88/88 [==============================] - 1s 7ms/step - loss: 0.0016 - mse: 0.0016 - val_loss: 0.0016 - val_mse: 0.0016\n",
      "Epoch 13/300\n",
      "88/88 [==============================] - 0s 4ms/step - loss: 0.0014 - mse: 0.0014 - val_loss: 0.0014 - val_mse: 0.0014\n",
      "Epoch 14/300\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.0012 - mse: 0.0012 - val_loss: 0.0012 - val_mse: 0.0012\n",
      "Epoch 15/300\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.0011 - mse: 0.0011 - val_loss: 0.0011 - val_mse: 0.0011\n",
      "Epoch 16/300\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 9.6594e-04 - mse: 9.6594e-04 - val_loss: 9.9835e-04 - val_mse: 9.9835e-04\n",
      "Epoch 17/300\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 8.7047e-04 - mse: 8.7047e-04 - val_loss: 9.0324e-04 - val_mse: 9.0324e-04\n",
      "Epoch 18/300\n",
      "88/88 [==============================] - 0s 3ms/step - loss: 7.8368e-04 - mse: 7.8368e-04 - val_loss: 8.1985e-04 - val_mse: 8.1985e-04\n",
      "Epoch 19/300\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 7.1053e-04 - mse: 7.1053e-04 - val_loss: 7.4164e-04 - val_mse: 7.4164e-04\n",
      "Epoch 20/300\n",
      "88/88 [==============================] - 0s 4ms/step - loss: 6.4640e-04 - mse: 6.4640e-04 - val_loss: 6.7932e-04 - val_mse: 6.7932e-04\n",
      "Epoch 21/300\n",
      "88/88 [==============================] - 1s 6ms/step - loss: 5.8637e-04 - mse: 5.8637e-04 - val_loss: 6.2119e-04 - val_mse: 6.2119e-04\n",
      "Epoch 22/300\n",
      "88/88 [==============================] - 1s 6ms/step - loss: 5.3505e-04 - mse: 5.3505e-04 - val_loss: 5.6714e-04 - val_mse: 5.6714e-04\n",
      "Epoch 23/300\n",
      "88/88 [==============================] - 1s 6ms/step - loss: 4.8757e-04 - mse: 4.8757e-04 - val_loss: 5.2008e-04 - val_mse: 5.2008e-04\n",
      "Epoch 24/300\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 4.4704e-04 - mse: 4.4704e-04 - val_loss: 4.7760e-04 - val_mse: 4.7760e-04\n",
      "Epoch 25/300\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 4.0932e-04 - mse: 4.0932e-04 - val_loss: 4.3946e-04 - val_mse: 4.3946e-04\n",
      "Epoch 26/300\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 3.7458e-04 - mse: 3.7458e-04 - val_loss: 4.0640e-04 - val_mse: 4.0640e-04\n",
      "Epoch 27/300\n",
      "88/88 [==============================] - 1s 7ms/step - loss: 3.4281e-04 - mse: 3.4281e-04 - val_loss: 3.7340e-04 - val_mse: 3.7340e-04\n",
      "Epoch 28/300\n",
      "88/88 [==============================] - 1s 8ms/step - loss: 3.1549e-04 - mse: 3.1549e-04 - val_loss: 3.4549e-04 - val_mse: 3.4549e-04\n",
      "Epoch 29/300\n",
      "88/88 [==============================] - 1s 8ms/step - loss: 2.9115e-04 - mse: 2.9115e-04 - val_loss: 3.2337e-04 - val_mse: 3.2337e-04\n",
      "Epoch 30/300\n",
      "88/88 [==============================] - 1s 6ms/step - loss: 2.6729e-04 - mse: 2.6729e-04 - val_loss: 2.9457e-04 - val_mse: 2.9457e-04\n",
      "Epoch 31/300\n",
      "88/88 [==============================] - 1s 6ms/step - loss: 2.4491e-04 - mse: 2.4491e-04 - val_loss: 2.7241e-04 - val_mse: 2.7241e-04\n",
      "Epoch 32/300\n",
      "88/88 [==============================] - 1s 6ms/step - loss: 2.2742e-04 - mse: 2.2742e-04 - val_loss: 2.5192e-04 - val_mse: 2.5192e-04\n",
      "Epoch 33/300\n",
      "88/88 [==============================] - 1s 8ms/step - loss: 2.0746e-04 - mse: 2.0746e-04 - val_loss: 2.3404e-04 - val_mse: 2.3404e-04\n",
      "Epoch 34/300\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 1.9192e-04 - mse: 1.9192e-04 - val_loss: 2.1831e-04 - val_mse: 2.1831e-04\n",
      "Epoch 35/300\n",
      "88/88 [==============================] - 1s 9ms/step - loss: 1.7640e-04 - mse: 1.7640e-04 - val_loss: 2.0425e-04 - val_mse: 2.0425e-04\n",
      "Epoch 36/300\n",
      "88/88 [==============================] - 1s 7ms/step - loss: 1.6133e-04 - mse: 1.6133e-04 - val_loss: 1.8959e-04 - val_mse: 1.8959e-04\n",
      "Epoch 37/300\n",
      "88/88 [==============================] - 0s 4ms/step - loss: 1.4946e-04 - mse: 1.4946e-04 - val_loss: 1.7236e-04 - val_mse: 1.7236e-04\n",
      "Epoch 38/300\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 1.3498e-04 - mse: 1.3498e-04 - val_loss: 1.6213e-04 - val_mse: 1.6213e-04\n",
      "Epoch 39/300\n",
      "88/88 [==============================] - 0s 6ms/step - loss: 1.2253e-04 - mse: 1.2253e-04 - val_loss: 1.5004e-04 - val_mse: 1.5004e-04\n",
      "Epoch 40/300\n",
      "88/88 [==============================] - 0s 3ms/step - loss: 1.1157e-04 - mse: 1.1157e-04 - val_loss: 1.3595e-04 - val_mse: 1.3595e-04\n",
      "Epoch 41/300\n",
      "88/88 [==============================] - 0s 4ms/step - loss: 1.0077e-04 - mse: 1.0077e-04 - val_loss: 1.2491e-04 - val_mse: 1.2491e-04\n",
      "Epoch 42/300\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 9.1361e-05 - mse: 9.1361e-05 - val_loss: 1.1492e-04 - val_mse: 1.1492e-04\n",
      "Epoch 43/300\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 8.2390e-05 - mse: 8.2390e-05 - val_loss: 1.0744e-04 - val_mse: 1.0744e-04\n",
      "Epoch 44/300\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 7.3446e-05 - mse: 7.3446e-05 - val_loss: 1.0025e-04 - val_mse: 1.0025e-04\n",
      "Epoch 45/300\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 6.6589e-05 - mse: 6.6589e-05 - val_loss: 9.1090e-05 - val_mse: 9.1090e-05\n",
      "Epoch 46/300\n",
      "88/88 [==============================] - 1s 6ms/step - loss: 6.1229e-05 - mse: 6.1229e-05 - val_loss: 8.5303e-05 - val_mse: 8.5303e-05\n",
      "Epoch 47/300\n",
      "88/88 [==============================] - 1s 7ms/step - loss: 5.5121e-05 - mse: 5.5121e-05 - val_loss: 8.0318e-05 - val_mse: 8.0318e-05\n",
      "Epoch 48/300\n",
      "88/88 [==============================] - 1s 7ms/step - loss: 5.1031e-05 - mse: 5.1031e-05 - val_loss: 7.7421e-05 - val_mse: 7.7421e-05\n",
      "Epoch 49/300\n",
      "88/88 [==============================] - 1s 7ms/step - loss: 4.6956e-05 - mse: 4.6956e-05 - val_loss: 7.4790e-05 - val_mse: 7.4790e-05\n",
      "Epoch 50/300\n",
      "88/88 [==============================] - 1s 9ms/step - loss: 4.5063e-05 - mse: 4.5063e-05 - val_loss: 6.9928e-05 - val_mse: 6.9928e-05\n",
      "Epoch 51/300\n",
      "88/88 [==============================] - 1s 8ms/step - loss: 4.1115e-05 - mse: 4.1115e-05 - val_loss: 6.5092e-05 - val_mse: 6.5092e-05\n",
      "Epoch 52/300\n",
      "88/88 [==============================] - 1s 8ms/step - loss: 3.8575e-05 - mse: 3.8575e-05 - val_loss: 6.2857e-05 - val_mse: 6.2857e-05\n",
      "Epoch 53/300\n",
      "88/88 [==============================] - 1s 8ms/step - loss: 3.6284e-05 - mse: 3.6284e-05 - val_loss: 6.0468e-05 - val_mse: 6.0468e-05\n",
      "Epoch 54/300\n",
      "88/88 [==============================] - 1s 8ms/step - loss: 3.4633e-05 - mse: 3.4633e-05 - val_loss: 5.8507e-05 - val_mse: 5.8507e-05\n",
      "Epoch 55/300\n",
      "88/88 [==============================] - 1s 8ms/step - loss: 3.2854e-05 - mse: 3.2854e-05 - val_loss: 5.6741e-05 - val_mse: 5.6741e-05\n",
      "Epoch 56/300\n",
      "88/88 [==============================] - 1s 9ms/step - loss: 3.1554e-05 - mse: 3.1554e-05 - val_loss: 5.4884e-05 - val_mse: 5.4884e-05\n",
      "Epoch 57/300\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 2.9940e-05 - mse: 2.9940e-05 - val_loss: 5.4100e-05 - val_mse: 5.4100e-05\n",
      "Epoch 58/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88/88 [==============================] - 1s 8ms/step - loss: 2.9190e-05 - mse: 2.9190e-05 - val_loss: 5.2164e-05 - val_mse: 5.2164e-05\n",
      "Epoch 59/300\n",
      "88/88 [==============================] - 1s 8ms/step - loss: 2.7904e-05 - mse: 2.7904e-05 - val_loss: 5.4165e-05 - val_mse: 5.4165e-05\n",
      "Epoch 60/300\n",
      "88/88 [==============================] - 1s 8ms/step - loss: 2.7137e-05 - mse: 2.7137e-05 - val_loss: 5.1160e-05 - val_mse: 5.1160e-05\n",
      "Epoch 61/300\n",
      "88/88 [==============================] - 1s 8ms/step - loss: 2.5709e-05 - mse: 2.5709e-05 - val_loss: 5.0990e-05 - val_mse: 5.0990e-05\n",
      "Epoch 62/300\n",
      "88/88 [==============================] - 1s 8ms/step - loss: 2.4985e-05 - mse: 2.4985e-05 - val_loss: 4.8620e-05 - val_mse: 4.8620e-05\n",
      "Epoch 63/300\n",
      "88/88 [==============================] - 1s 8ms/step - loss: 2.4686e-05 - mse: 2.4686e-05 - val_loss: 4.8625e-05 - val_mse: 4.8625e-05\n",
      "Epoch 64/300\n",
      "88/88 [==============================] - 1s 8ms/step - loss: 2.4987e-05 - mse: 2.4987e-05 - val_loss: 4.7245e-05 - val_mse: 4.7245e-05\n",
      "Epoch 65/300\n",
      "88/88 [==============================] - 1s 8ms/step - loss: 2.3463e-05 - mse: 2.3463e-05 - val_loss: 4.6026e-05 - val_mse: 4.6026e-05\n",
      "Epoch 66/300\n",
      "88/88 [==============================] - 1s 8ms/step - loss: 2.2719e-05 - mse: 2.2719e-05 - val_loss: 4.7834e-05 - val_mse: 4.7834e-05\n",
      "Epoch 67/300\n",
      "88/88 [==============================] - 1s 8ms/step - loss: 2.1535e-05 - mse: 2.1535e-05 - val_loss: 4.6403e-05 - val_mse: 4.6403e-05\n",
      "Epoch 68/300\n",
      "88/88 [==============================] - 1s 8ms/step - loss: 2.0899e-05 - mse: 2.0899e-05 - val_loss: 4.5377e-05 - val_mse: 4.5377e-05\n",
      "Epoch 69/300\n",
      "88/88 [==============================] - 1s 8ms/step - loss: 1.9990e-05 - mse: 1.9990e-05 - val_loss: 4.3910e-05 - val_mse: 4.3910e-05\n",
      "Epoch 70/300\n",
      "88/88 [==============================] - 1s 8ms/step - loss: 1.9984e-05 - mse: 1.9984e-05 - val_loss: 4.4259e-05 - val_mse: 4.4259e-05\n",
      "Epoch 71/300\n",
      "88/88 [==============================] - 1s 8ms/step - loss: 1.9445e-05 - mse: 1.9445e-05 - val_loss: 4.2850e-05 - val_mse: 4.2850e-05\n",
      "Epoch 72/300\n",
      "88/88 [==============================] - 1s 8ms/step - loss: 1.9520e-05 - mse: 1.9520e-05 - val_loss: 4.3658e-05 - val_mse: 4.3658e-05\n",
      "Epoch 73/300\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 1.8508e-05 - mse: 1.8508e-05 - val_loss: 4.2299e-05 - val_mse: 4.2299e-05\n",
      "Epoch 74/300\n",
      "88/88 [==============================] - 1s 7ms/step - loss: 1.8117e-05 - mse: 1.8117e-05 - val_loss: 4.4956e-05 - val_mse: 4.4956e-05\n",
      "Epoch 75/300\n",
      "88/88 [==============================] - 1s 8ms/step - loss: 1.8227e-05 - mse: 1.8227e-05 - val_loss: 4.3102e-05 - val_mse: 4.3102e-05\n",
      "Epoch 76/300\n",
      "88/88 [==============================] - 1s 8ms/step - loss: 1.7274e-05 - mse: 1.7274e-05 - val_loss: 4.2935e-05 - val_mse: 4.2935e-05\n",
      "Epoch 77/300\n",
      "88/88 [==============================] - 1s 8ms/step - loss: 1.6696e-05 - mse: 1.6696e-05 - val_loss: 4.0857e-05 - val_mse: 4.0857e-05\n",
      "Epoch 78/300\n",
      "88/88 [==============================] - 1s 8ms/step - loss: 1.6262e-05 - mse: 1.6262e-05 - val_loss: 4.2144e-05 - val_mse: 4.2144e-05\n",
      "Epoch 79/300\n",
      "88/88 [==============================] - 1s 8ms/step - loss: 1.5948e-05 - mse: 1.5948e-05 - val_loss: 4.2111e-05 - val_mse: 4.2111e-05\n",
      "Epoch 80/300\n",
      "88/88 [==============================] - 1s 9ms/step - loss: 1.6250e-05 - mse: 1.6250e-05 - val_loss: 4.3200e-05 - val_mse: 4.3200e-05\n",
      "Epoch 81/300\n",
      "88/88 [==============================] - 1s 9ms/step - loss: 1.5727e-05 - mse: 1.5727e-05 - val_loss: 4.1637e-05 - val_mse: 4.1637e-05\n",
      "Epoch 82/300\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 1.5703e-05 - mse: 1.5703e-05 - val_loss: 4.3059e-05 - val_mse: 4.3059e-05\n",
      "Epoch 83/300\n",
      "88/88 [==============================] - 1s 7ms/step - loss: 1.5636e-05 - mse: 1.5636e-05 - val_loss: 4.0375e-05 - val_mse: 4.0375e-05\n",
      "Epoch 84/300\n",
      "88/88 [==============================] - 1s 8ms/step - loss: 1.5235e-05 - mse: 1.5235e-05 - val_loss: 3.8911e-05 - val_mse: 3.8911e-05\n",
      "Epoch 85/300\n",
      "88/88 [==============================] - 1s 8ms/step - loss: 1.4489e-05 - mse: 1.4489e-05 - val_loss: 3.8591e-05 - val_mse: 3.8591e-05\n",
      "Epoch 86/300\n",
      "88/88 [==============================] - 1s 8ms/step - loss: 1.4121e-05 - mse: 1.4121e-05 - val_loss: 3.8631e-05 - val_mse: 3.8631e-05\n",
      "Epoch 87/300\n",
      "88/88 [==============================] - 1s 8ms/step - loss: 1.3869e-05 - mse: 1.3869e-05 - val_loss: 3.8736e-05 - val_mse: 3.8736e-05\n",
      "Epoch 88/300\n",
      "88/88 [==============================] - 0s 3ms/step - loss: 1.3511e-05 - mse: 1.3511e-05 - val_loss: 3.8394e-05 - val_mse: 3.8394e-05\n",
      "Epoch 89/300\n",
      "88/88 [==============================] - 0s 6ms/step - loss: 1.3132e-05 - mse: 1.3132e-05 - val_loss: 3.7543e-05 - val_mse: 3.7543e-05\n",
      "Epoch 90/300\n",
      "88/88 [==============================] - 1s 7ms/step - loss: 1.2774e-05 - mse: 1.2774e-05 - val_loss: 3.7547e-05 - val_mse: 3.7547e-05\n",
      "Epoch 91/300\n",
      "88/88 [==============================] - 1s 8ms/step - loss: 1.3853e-05 - mse: 1.3853e-05 - val_loss: 3.8515e-05 - val_mse: 3.8515e-05\n",
      "Epoch 92/300\n",
      "88/88 [==============================] - 1s 8ms/step - loss: 1.3016e-05 - mse: 1.3016e-05 - val_loss: 3.6839e-05 - val_mse: 3.6839e-05\n",
      "Epoch 93/300\n",
      "88/88 [==============================] - 1s 7ms/step - loss: 1.2792e-05 - mse: 1.2792e-05 - val_loss: 3.6258e-05 - val_mse: 3.6258e-05\n",
      "Epoch 94/300\n",
      "88/88 [==============================] - 1s 8ms/step - loss: 1.2566e-05 - mse: 1.2566e-05 - val_loss: 3.7716e-05 - val_mse: 3.7716e-05\n",
      "Epoch 95/300\n",
      "88/88 [==============================] - 1s 8ms/step - loss: 1.2071e-05 - mse: 1.2071e-05 - val_loss: 3.8250e-05 - val_mse: 3.8250e-05\n",
      "Epoch 96/300\n",
      "88/88 [==============================] - 1s 8ms/step - loss: 1.1977e-05 - mse: 1.1977e-05 - val_loss: 3.8426e-05 - val_mse: 3.8426e-05\n",
      "Epoch 97/300\n",
      "88/88 [==============================] - 1s 8ms/step - loss: 1.1597e-05 - mse: 1.1597e-05 - val_loss: 3.5200e-05 - val_mse: 3.5200e-05\n",
      "Epoch 98/300\n",
      "88/88 [==============================] - 1s 8ms/step - loss: 1.1411e-05 - mse: 1.1411e-05 - val_loss: 3.4736e-05 - val_mse: 3.4736e-05\n",
      "Epoch 99/300\n",
      "88/88 [==============================] - 1s 7ms/step - loss: 1.1365e-05 - mse: 1.1365e-05 - val_loss: 3.4183e-05 - val_mse: 3.4183e-05\n",
      "Epoch 100/300\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 1.0771e-05 - mse: 1.0771e-05 - val_loss: 3.4639e-05 - val_mse: 3.4639e-05\n",
      "Epoch 101/300\n",
      "88/88 [==============================] - 0s 6ms/step - loss: 1.0619e-05 - mse: 1.0619e-05 - val_loss: 3.3642e-05 - val_mse: 3.3642e-05\n",
      "Epoch 102/300\n",
      "88/88 [==============================] - 1s 6ms/step - loss: 1.0300e-05 - mse: 1.0300e-05 - val_loss: 3.4525e-05 - val_mse: 3.4525e-05\n",
      "Epoch 103/300\n",
      "88/88 [==============================] - 1s 8ms/step - loss: 1.0104e-05 - mse: 1.0104e-05 - val_loss: 3.1962e-05 - val_mse: 3.1962e-05\n",
      "Epoch 104/300\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 1.0336e-05 - mse: 1.0336e-05 - val_loss: 3.5914e-05 - val_mse: 3.5914e-05\n",
      "Epoch 105/300\n",
      "88/88 [==============================] - 1s 7ms/step - loss: 1.0222e-05 - mse: 1.0222e-05 - val_loss: 3.3590e-05 - val_mse: 3.3590e-05\n",
      "Epoch 106/300\n",
      "88/88 [==============================] - 1s 6ms/step - loss: 9.6414e-06 - mse: 9.6414e-06 - val_loss: 3.4335e-05 - val_mse: 3.4335e-05\n",
      "Epoch 107/300\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 9.3257e-06 - mse: 9.3257e-06 - val_loss: 3.1761e-05 - val_mse: 3.1761e-05\n",
      "Epoch 108/300\n",
      "88/88 [==============================] - 1s 6ms/step - loss: 9.0511e-06 - mse: 9.0511e-06 - val_loss: 3.0994e-05 - val_mse: 3.0994e-05\n",
      "Epoch 109/300\n",
      "88/88 [==============================] - 1s 7ms/step - loss: 9.0863e-06 - mse: 9.0863e-06 - val_loss: 2.9583e-05 - val_mse: 2.9583e-05\n",
      "Epoch 110/300\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 8.5313e-06 - mse: 8.5313e-06 - val_loss: 2.9887e-05 - val_mse: 2.9887e-05\n",
      "Epoch 111/300\n",
      "88/88 [==============================] - 1s 7ms/step - loss: 9.0367e-06 - mse: 9.0367e-06 - val_loss: 2.8891e-05 - val_mse: 2.8891e-05\n",
      "Epoch 112/300\n",
      "88/88 [==============================] - 1s 8ms/step - loss: 8.0630e-06 - mse: 8.0630e-06 - val_loss: 3.0900e-05 - val_mse: 3.0900e-05\n",
      "Epoch 113/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88/88 [==============================] - 1s 6ms/step - loss: 7.9024e-06 - mse: 7.9024e-06 - val_loss: 2.8915e-05 - val_mse: 2.8915e-05\n",
      "Epoch 114/300\n",
      "88/88 [==============================] - 1s 8ms/step - loss: 7.4732e-06 - mse: 7.4732e-06 - val_loss: 2.7633e-05 - val_mse: 2.7633e-05\n",
      "Epoch 115/300\n",
      "88/88 [==============================] - 1s 8ms/step - loss: 7.1462e-06 - mse: 7.1462e-06 - val_loss: 2.6307e-05 - val_mse: 2.6307e-05\n",
      "Epoch 116/300\n",
      "88/88 [==============================] - 1s 8ms/step - loss: 7.3651e-06 - mse: 7.3651e-06 - val_loss: 2.6182e-05 - val_mse: 2.6182e-05\n",
      "Epoch 117/300\n",
      "88/88 [==============================] - 1s 8ms/step - loss: 7.0220e-06 - mse: 7.0220e-06 - val_loss: 2.7300e-05 - val_mse: 2.7300e-05\n",
      "Epoch 118/300\n",
      "88/88 [==============================] - 1s 9ms/step - loss: 7.0810e-06 - mse: 7.0810e-06 - val_loss: 2.6242e-05 - val_mse: 2.6242e-05\n",
      "Epoch 119/300\n",
      "88/88 [==============================] - 1s 6ms/step - loss: 6.5006e-06 - mse: 6.5006e-06 - val_loss: 2.5400e-05 - val_mse: 2.5400e-05\n",
      "Epoch 120/300\n",
      "88/88 [==============================] - 1s 8ms/step - loss: 6.2891e-06 - mse: 6.2891e-06 - val_loss: 2.4665e-05 - val_mse: 2.4665e-05\n",
      "Epoch 121/300\n",
      "88/88 [==============================] - 1s 7ms/step - loss: 6.2387e-06 - mse: 6.2387e-06 - val_loss: 2.4017e-05 - val_mse: 2.4017e-05\n",
      "Epoch 122/300\n",
      "88/88 [==============================] - 1s 9ms/step - loss: 5.8058e-06 - mse: 5.8058e-06 - val_loss: 2.3769e-05 - val_mse: 2.3769e-05\n",
      "Epoch 123/300\n",
      "88/88 [==============================] - 1s 9ms/step - loss: 5.6468e-06 - mse: 5.6468e-06 - val_loss: 2.0596e-05 - val_mse: 2.0596e-05\n",
      "Epoch 124/300\n",
      "88/88 [==============================] - 1s 6ms/step - loss: 5.1231e-06 - mse: 5.1231e-06 - val_loss: 1.9838e-05 - val_mse: 1.9838e-05\n",
      "Epoch 125/300\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 4.8395e-06 - mse: 4.8395e-06 - val_loss: 1.8075e-05 - val_mse: 1.8075e-05\n",
      "Epoch 126/300\n",
      "88/88 [==============================] - 1s 7ms/step - loss: 4.6664e-06 - mse: 4.6664e-06 - val_loss: 1.7666e-05 - val_mse: 1.7666e-05\n",
      "Epoch 127/300\n",
      "88/88 [==============================] - 1s 8ms/step - loss: 4.3474e-06 - mse: 4.3474e-06 - val_loss: 1.7089e-05 - val_mse: 1.7089e-05\n",
      "Epoch 128/300\n",
      "88/88 [==============================] - 1s 7ms/step - loss: 4.2098e-06 - mse: 4.2098e-06 - val_loss: 1.8685e-05 - val_mse: 1.8685e-05\n",
      "Epoch 129/300\n",
      "88/88 [==============================] - 1s 8ms/step - loss: 3.9913e-06 - mse: 3.9913e-06 - val_loss: 1.6955e-05 - val_mse: 1.6955e-05\n",
      "Epoch 130/300\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 3.7778e-06 - mse: 3.7778e-06 - val_loss: 1.7138e-05 - val_mse: 1.7138e-05\n",
      "Epoch 131/300\n",
      "88/88 [==============================] - 1s 7ms/step - loss: 3.6984e-06 - mse: 3.6984e-06 - val_loss: 1.5881e-05 - val_mse: 1.5881e-05\n",
      "Epoch 132/300\n",
      "88/88 [==============================] - 1s 6ms/step - loss: 3.3949e-06 - mse: 3.3949e-06 - val_loss: 1.5317e-05 - val_mse: 1.5317e-05\n",
      "Epoch 133/300\n",
      "88/88 [==============================] - 1s 7ms/step - loss: 3.0663e-06 - mse: 3.0663e-06 - val_loss: 1.4714e-05 - val_mse: 1.4714e-05\n",
      "Epoch 134/300\n",
      "88/88 [==============================] - 1s 6ms/step - loss: 2.8668e-06 - mse: 2.8668e-06 - val_loss: 1.4668e-05 - val_mse: 1.4668e-05\n",
      "Epoch 135/300\n",
      "88/88 [==============================] - 1s 8ms/step - loss: 2.5154e-06 - mse: 2.5154e-06 - val_loss: 1.3228e-05 - val_mse: 1.3228e-05\n",
      "Epoch 136/300\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 2.3342e-06 - mse: 2.3342e-06 - val_loss: 1.3069e-05 - val_mse: 1.3069e-05\n",
      "Epoch 137/300\n",
      "88/88 [==============================] - 1s 7ms/step - loss: 2.0549e-06 - mse: 2.0549e-06 - val_loss: 1.2597e-05 - val_mse: 1.2597e-05\n",
      "Epoch 138/300\n",
      "88/88 [==============================] - 1s 7ms/step - loss: 1.9329e-06 - mse: 1.9329e-06 - val_loss: 1.2309e-05 - val_mse: 1.2309e-05\n",
      "Epoch 139/300\n",
      "88/88 [==============================] - 0s 4ms/step - loss: 1.8018e-06 - mse: 1.8018e-06 - val_loss: 1.2169e-05 - val_mse: 1.2169e-05\n",
      "Epoch 140/300\n",
      "88/88 [==============================] - 0s 3ms/step - loss: 1.5390e-06 - mse: 1.5390e-06 - val_loss: 1.1991e-05 - val_mse: 1.1991e-05\n",
      "Epoch 141/300\n",
      "88/88 [==============================] - 0s 4ms/step - loss: 1.7177e-06 - mse: 1.7177e-06 - val_loss: 1.1484e-05 - val_mse: 1.1484e-05\n",
      "Epoch 142/300\n",
      "88/88 [==============================] - 0s 3ms/step - loss: 1.2917e-06 - mse: 1.2917e-06 - val_loss: 1.1345e-05 - val_mse: 1.1345e-05\n",
      "Epoch 143/300\n",
      "88/88 [==============================] - 0s 4ms/step - loss: 1.0870e-06 - mse: 1.0870e-06 - val_loss: 1.1303e-05 - val_mse: 1.1303e-05\n",
      "Epoch 144/300\n",
      "88/88 [==============================] - 0s 4ms/step - loss: 9.8890e-07 - mse: 9.8890e-07 - val_loss: 1.1256e-05 - val_mse: 1.1256e-05\n",
      "Epoch 145/300\n",
      "88/88 [==============================] - 1s 6ms/step - loss: 8.3017e-07 - mse: 8.3017e-07 - val_loss: 1.1052e-05 - val_mse: 1.1052e-05\n",
      "Epoch 146/300\n",
      "88/88 [==============================] - 0s 6ms/step - loss: 6.7572e-07 - mse: 6.7572e-07 - val_loss: 1.0632e-05 - val_mse: 1.0632e-05\n",
      "Epoch 147/300\n",
      "88/88 [==============================] - 1s 6ms/step - loss: 6.5241e-07 - mse: 6.5241e-07 - val_loss: 1.0556e-05 - val_mse: 1.0556e-05\n",
      "Epoch 148/300\n",
      "88/88 [==============================] - 1s 6ms/step - loss: 4.7977e-07 - mse: 4.7977e-07 - val_loss: 1.0217e-05 - val_mse: 1.0217e-05\n",
      "Epoch 149/300\n",
      "88/88 [==============================] - 0s 4ms/step - loss: 4.3835e-07 - mse: 4.3835e-07 - val_loss: 1.0188e-05 - val_mse: 1.0188e-05\n",
      "Epoch 150/300\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 3.3014e-07 - mse: 3.3014e-07 - val_loss: 1.0084e-05 - val_mse: 1.0084e-05\n",
      "Epoch 151/300\n",
      "88/88 [==============================] - 1s 6ms/step - loss: 2.6627e-07 - mse: 2.6627e-07 - val_loss: 9.9770e-06 - val_mse: 9.9770e-06\n",
      "Epoch 152/300\n",
      "88/88 [==============================] - 0s 6ms/step - loss: 2.9364e-07 - mse: 2.9364e-07 - val_loss: 1.0102e-05 - val_mse: 1.0102e-05\n",
      "Epoch 153/300\n",
      "88/88 [==============================] - 1s 6ms/step - loss: 2.3684e-07 - mse: 2.3684e-07 - val_loss: 9.8028e-06 - val_mse: 9.8028e-06\n",
      "Epoch 154/300\n",
      "88/88 [==============================] - 1s 8ms/step - loss: 2.8416e-07 - mse: 2.8416e-07 - val_loss: 9.6525e-06 - val_mse: 9.6525e-06\n",
      "Epoch 155/300\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 1.6667e-07 - mse: 1.6667e-07 - val_loss: 9.4254e-06 - val_mse: 9.4254e-06\n",
      "Epoch 156/300\n",
      "88/88 [==============================] - 1s 6ms/step - loss: 1.6359e-07 - mse: 1.6359e-07 - val_loss: 9.2740e-06 - val_mse: 9.2740e-06\n",
      "Epoch 157/300\n",
      "88/88 [==============================] - 1s 6ms/step - loss: 1.1705e-07 - mse: 1.1705e-07 - val_loss: 9.2227e-06 - val_mse: 9.2227e-06\n",
      "Epoch 158/300\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 1.1144e-07 - mse: 1.1144e-07 - val_loss: 9.1357e-06 - val_mse: 9.1357e-06\n",
      "Epoch 159/300\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 9.9593e-08 - mse: 9.9593e-08 - val_loss: 9.2659e-06 - val_mse: 9.2659e-06\n",
      "Epoch 160/300\n",
      "88/88 [==============================] - 1s 6ms/step - loss: 1.8134e-07 - mse: 1.8134e-07 - val_loss: 9.1579e-06 - val_mse: 9.1579e-06\n",
      "Epoch 161/300\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 5.0453e-08 - mse: 5.0453e-08 - val_loss: 9.1212e-06 - val_mse: 9.1212e-06\n",
      "Epoch 162/300\n",
      "88/88 [==============================] - 1s 7ms/step - loss: 7.3723e-08 - mse: 7.3723e-08 - val_loss: 9.0213e-06 - val_mse: 9.0213e-06\n",
      "Epoch 163/300\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 4.3563e-08 - mse: 4.3563e-08 - val_loss: 8.9725e-06 - val_mse: 8.9725e-06\n",
      "Epoch 164/300\n",
      "88/88 [==============================] - 0s 6ms/step - loss: 1.0680e-07 - mse: 1.0680e-07 - val_loss: 8.8899e-06 - val_mse: 8.8899e-06\n",
      "Epoch 165/300\n",
      "88/88 [==============================] - 1s 6ms/step - loss: 6.1842e-08 - mse: 6.1842e-08 - val_loss: 9.0477e-06 - val_mse: 9.0477e-06\n",
      "Epoch 166/300\n",
      "88/88 [==============================] - 1s 6ms/step - loss: 5.7393e-08 - mse: 5.7393e-08 - val_loss: 9.0309e-06 - val_mse: 9.0309e-06\n",
      "Epoch 167/300\n",
      "88/88 [==============================] - 1s 6ms/step - loss: 7.3577e-08 - mse: 7.3577e-08 - val_loss: 8.9403e-06 - val_mse: 8.9403e-06\n",
      "Epoch 168/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88/88 [==============================] - 0s 6ms/step - loss: 5.7231e-08 - mse: 5.7231e-08 - val_loss: 8.9407e-06 - val_mse: 8.9407e-06\n",
      "Epoch 169/300\n",
      "88/88 [==============================] - 1s 8ms/step - loss: 5.6415e-08 - mse: 5.6415e-08 - val_loss: 8.8728e-06 - val_mse: 8.8728e-06\n",
      "Epoch 170/300\n",
      "88/88 [==============================] - 1s 9ms/step - loss: 1.5401e-07 - mse: 1.5401e-07 - val_loss: 8.8549e-06 - val_mse: 8.8549e-06\n",
      "Epoch 171/300\n",
      "88/88 [==============================] - 1s 9ms/step - loss: 2.8127e-08 - mse: 2.8127e-08 - val_loss: 8.8936e-06 - val_mse: 8.8936e-06\n",
      "Epoch 172/300\n",
      "88/88 [==============================] - 1s 8ms/step - loss: 1.1010e-07 - mse: 1.1010e-07 - val_loss: 9.8401e-06 - val_mse: 9.8401e-06\n",
      "Epoch 173/300\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 4.1898e-07 - mse: 4.1898e-07 - val_loss: 8.9115e-06 - val_mse: 8.9115e-06\n",
      "Epoch 174/300\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 1.1951e-08 - mse: 1.1951e-08 - val_loss: 8.9909e-06 - val_mse: 8.9909e-06\n",
      "Epoch 175/300\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 1.3620e-08 - mse: 1.3620e-08 - val_loss: 9.0231e-06 - val_mse: 9.0231e-06\n",
      "Epoch 176/300\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 1.8105e-08 - mse: 1.8105e-08 - val_loss: 9.0071e-06 - val_mse: 9.0071e-06\n",
      "Epoch 177/300\n",
      "88/88 [==============================] - 1s 6ms/step - loss: 3.1724e-08 - mse: 3.1724e-08 - val_loss: 8.9865e-06 - val_mse: 8.9865e-06\n",
      "Epoch 178/300\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 2.4598e-07 - mse: 2.4598e-07 - val_loss: 9.1894e-06 - val_mse: 9.1894e-06\n",
      "Epoch 179/300\n",
      "88/88 [==============================] - 1s 8ms/step - loss: 7.0737e-08 - mse: 7.0737e-08 - val_loss: 9.0087e-06 - val_mse: 9.0087e-06\n",
      "Epoch 180/300\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 2.0246e-08 - mse: 2.0246e-08 - val_loss: 9.0424e-06 - val_mse: 9.0424e-06\n",
      "Epoch 181/300\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 1.8957e-08 - mse: 1.8957e-08 - val_loss: 9.0063e-06 - val_mse: 9.0063e-06\n",
      "Epoch 182/300\n",
      "88/88 [==============================] - 0s 3ms/step - loss: 1.8155e-07 - mse: 1.8155e-07 - val_loss: 9.0541e-06 - val_mse: 9.0541e-06\n",
      "Epoch 183/300\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 8.5285e-08 - mse: 8.5285e-08 - val_loss: 9.4210e-06 - val_mse: 9.4210e-06\n",
      "Epoch 184/300\n",
      "88/88 [==============================] - 0s 6ms/step - loss: 1.7741e-07 - mse: 1.7741e-07 - val_loss: 9.1340e-06 - val_mse: 9.1340e-06\n",
      "Epoch 185/300\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 1.6187e-07 - mse: 1.6187e-07 - val_loss: 9.1759e-06 - val_mse: 9.1759e-06\n",
      "Epoch 186/300\n",
      "88/88 [==============================] - 1s 6ms/step - loss: 2.7147e-08 - mse: 2.7147e-08 - val_loss: 9.1387e-06 - val_mse: 9.1387e-06\n",
      "Epoch 187/300\n",
      "88/88 [==============================] - 0s 6ms/step - loss: 1.4629e-08 - mse: 1.4629e-08 - val_loss: 9.2934e-06 - val_mse: 9.2934e-06\n",
      "Epoch 188/300\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 2.2178e-07 - mse: 2.2178e-07 - val_loss: 9.7242e-06 - val_mse: 9.7242e-06\n",
      "Epoch 189/300\n",
      "88/88 [==============================] - 0s 4ms/step - loss: 1.2518e-07 - mse: 1.2518e-07 - val_loss: 9.2860e-06 - val_mse: 9.2860e-06\n",
      "Epoch 190/300\n",
      "88/88 [==============================] - 1s 6ms/step - loss: 1.8331e-08 - mse: 1.8331e-08 - val_loss: 9.2339e-06 - val_mse: 9.2339e-06\n",
      "Epoch 191/300\n",
      "88/88 [==============================] - 1s 7ms/step - loss: 5.3110e-08 - mse: 5.3110e-08 - val_loss: 9.2761e-06 - val_mse: 9.2761e-06\n",
      "Epoch 192/300\n",
      "88/88 [==============================] - 0s 4ms/step - loss: 7.4829e-08 - mse: 7.4829e-08 - val_loss: 9.4055e-06 - val_mse: 9.4055e-06\n",
      "Epoch 193/300\n",
      "88/88 [==============================] - 1s 6ms/step - loss: 2.7873e-07 - mse: 2.7873e-07 - val_loss: 9.3544e-06 - val_mse: 9.3544e-06\n",
      "Epoch 194/300\n",
      "88/88 [==============================] - 1s 6ms/step - loss: 5.1723e-08 - mse: 5.1723e-08 - val_loss: 9.3652e-06 - val_mse: 9.3652e-06\n",
      "Epoch 195/300\n",
      "88/88 [==============================] - 1s 6ms/step - loss: 2.9180e-08 - mse: 2.9180e-08 - val_loss: 9.2635e-06 - val_mse: 9.2635e-06\n",
      "Epoch 196/300\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 4.3009e-08 - mse: 4.3009e-08 - val_loss: 9.8054e-06 - val_mse: 9.8054e-06\n",
      "Epoch 197/300\n",
      "88/88 [==============================] - 1s 6ms/step - loss: 1.5059e-07 - mse: 1.5059e-07 - val_loss: 9.8572e-06 - val_mse: 9.8572e-06\n",
      "Epoch 198/300\n",
      "88/88 [==============================] - 1s 6ms/step - loss: 2.1755e-07 - mse: 2.1755e-07 - val_loss: 9.3898e-06 - val_mse: 9.3898e-06\n",
      "Epoch 199/300\n",
      "88/88 [==============================] - 1s 6ms/step - loss: 2.7822e-08 - mse: 2.7822e-08 - val_loss: 9.3753e-06 - val_mse: 9.3753e-06\n",
      "Epoch 200/300\n",
      "88/88 [==============================] - 1s 7ms/step - loss: 8.5459e-09 - mse: 8.5459e-09 - val_loss: 9.3500e-06 - val_mse: 9.3500e-06\n",
      "Epoch 201/300\n",
      "88/88 [==============================] - 1s 9ms/step - loss: 1.2618e-07 - mse: 1.2618e-07 - val_loss: 9.4589e-06 - val_mse: 9.4589e-06\n",
      "Epoch 202/300\n",
      "88/88 [==============================] - 1s 6ms/step - loss: 1.2588e-07 - mse: 1.2588e-07 - val_loss: 9.8417e-06 - val_mse: 9.8417e-06\n",
      "Epoch 203/300\n",
      "88/88 [==============================] - 0s 4ms/step - loss: 1.0526e-07 - mse: 1.0526e-07 - val_loss: 9.5619e-06 - val_mse: 9.5619e-06\n",
      "Epoch 204/300\n",
      "88/88 [==============================] - 1s 6ms/step - loss: 7.9784e-08 - mse: 7.9784e-08 - val_loss: 9.4570e-06 - val_mse: 9.4570e-06\n",
      "Epoch 205/300\n",
      "88/88 [==============================] - 0s 4ms/step - loss: 2.3111e-08 - mse: 2.3111e-08 - val_loss: 9.4623e-06 - val_mse: 9.4623e-06\n",
      "Epoch 206/300\n",
      "88/88 [==============================] - 0s 4ms/step - loss: 3.4522e-07 - mse: 3.4522e-07 - val_loss: 9.6455e-06 - val_mse: 9.6455e-06\n",
      "Epoch 207/300\n",
      "88/88 [==============================] - 0s 4ms/step - loss: 2.3462e-08 - mse: 2.3462e-08 - val_loss: 9.5285e-06 - val_mse: 9.5285e-06\n",
      "Epoch 208/300\n",
      "88/88 [==============================] - 1s 6ms/step - loss: 2.1464e-09 - mse: 2.1464e-09 - val_loss: 9.5249e-06 - val_mse: 9.5249e-06\n",
      "Epoch 209/300\n",
      "88/88 [==============================] - 1s 10ms/step - loss: 3.2982e-08 - mse: 3.2982e-08 - val_loss: 1.0878e-05 - val_mse: 1.0878e-05\n",
      "Epoch 210/300\n",
      "88/88 [==============================] - 1s 6ms/step - loss: 2.4224e-07 - mse: 2.4224e-07 - val_loss: 9.6292e-06 - val_mse: 9.6292e-06\n",
      "Epoch 211/300\n",
      "88/88 [==============================] - 1s 9ms/step - loss: 3.4853e-08 - mse: 3.4853e-08 - val_loss: 9.6104e-06 - val_mse: 9.6104e-06\n",
      "Epoch 212/300\n",
      "88/88 [==============================] - 1s 7ms/step - loss: 9.7033e-08 - mse: 9.7033e-08 - val_loss: 1.0044e-05 - val_mse: 1.0044e-05\n",
      "Epoch 213/300\n",
      "88/88 [==============================] - 1s 6ms/step - loss: 8.1448e-08 - mse: 8.1448e-08 - val_loss: 9.6605e-06 - val_mse: 9.6605e-06\n",
      "Epoch 214/300\n",
      "88/88 [==============================] - 1s 6ms/step - loss: 2.4714e-08 - mse: 2.4714e-08 - val_loss: 9.6760e-06 - val_mse: 9.6760e-06\n",
      "Epoch 215/300\n",
      "88/88 [==============================] - 1s 6ms/step - loss: 1.0982e-07 - mse: 1.0982e-07 - val_loss: 9.8169e-06 - val_mse: 9.8169e-06\n",
      "Epoch 216/300\n",
      "88/88 [==============================] - 1s 11ms/step - loss: 2.1484e-07 - mse: 2.1484e-07 - val_loss: 9.6935e-06 - val_mse: 9.6935e-06\n",
      "Epoch 217/300\n",
      "88/88 [==============================] - 1s 10ms/step - loss: 2.5823e-08 - mse: 2.5823e-08 - val_loss: 9.6942e-06 - val_mse: 9.6942e-06\n",
      "Epoch 218/300\n",
      "88/88 [==============================] - 1s 9ms/step - loss: 1.2501e-07 - mse: 1.2501e-07 - val_loss: 9.7539e-06 - val_mse: 9.7539e-06\n",
      "Epoch 219/300\n",
      "88/88 [==============================] - 1s 9ms/step - loss: 3.0079e-08 - mse: 3.0079e-08 - val_loss: 9.7395e-06 - val_mse: 9.7395e-06\n",
      "Epoch 220/300\n",
      "88/88 [==============================] - 1s 8ms/step - loss: 1.5334e-07 - mse: 1.5334e-07 - val_loss: 9.9200e-06 - val_mse: 9.9200e-06\n",
      "Epoch 221/300\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 2.9812e-07 - mse: 2.9812e-07 - val_loss: 9.9537e-06 - val_mse: 9.9537e-06\n",
      "Epoch 222/300\n",
      "88/88 [==============================] - 1s 6ms/step - loss: 3.5934e-08 - mse: 3.5934e-08 - val_loss: 9.9673e-06 - val_mse: 9.9673e-06\n",
      "Epoch 223/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88/88 [==============================] - 1s 8ms/step - loss: 1.5569e-07 - mse: 1.5569e-07 - val_loss: 1.0147e-05 - val_mse: 1.0147e-05\n",
      "Epoch 224/300\n",
      "88/88 [==============================] - 1s 9ms/step - loss: 9.6918e-08 - mse: 9.6918e-08 - val_loss: 1.0016e-05 - val_mse: 1.0016e-05\n",
      "Epoch 225/300\n",
      "88/88 [==============================] - 1s 10ms/step - loss: 3.9779e-09 - mse: 3.9779e-09 - val_loss: 9.9964e-06 - val_mse: 9.9964e-06\n",
      "Epoch 226/300\n",
      "88/88 [==============================] - 0s 3ms/step - loss: 3.5136e-09 - mse: 3.5136e-09 - val_loss: 9.9969e-06 - val_mse: 9.9969e-06\n",
      "Epoch 227/300\n",
      "88/88 [==============================] - 0s 3ms/step - loss: 3.4985e-08 - mse: 3.4985e-08 - val_loss: 1.0011e-05 - val_mse: 1.0011e-05\n",
      "Epoch 228/300\n",
      "88/88 [==============================] - 0s 3ms/step - loss: 1.9409e-07 - mse: 1.9409e-07 - val_loss: 1.0011e-05 - val_mse: 1.0011e-05\n",
      "Epoch 229/300\n",
      "88/88 [==============================] - 1s 9ms/step - loss: 5.7260e-08 - mse: 5.7260e-08 - val_loss: 1.0055e-05 - val_mse: 1.0055e-05\n",
      "Epoch 230/300\n",
      "88/88 [==============================] - 1s 9ms/step - loss: 6.5901e-08 - mse: 6.5901e-08 - val_loss: 1.0115e-05 - val_mse: 1.0115e-05\n",
      "Epoch 231/300\n",
      "88/88 [==============================] - 1s 9ms/step - loss: 1.6439e-07 - mse: 1.6439e-07 - val_loss: 1.0092e-05 - val_mse: 1.0092e-05\n",
      "Epoch 232/300\n",
      "88/88 [==============================] - 1s 9ms/step - loss: 3.9771e-08 - mse: 3.9771e-08 - val_loss: 1.0106e-05 - val_mse: 1.0106e-05\n",
      "Epoch 233/300\n",
      "88/88 [==============================] - 1s 8ms/step - loss: 1.0821e-07 - mse: 1.0821e-07 - val_loss: 1.0284e-05 - val_mse: 1.0284e-05\n",
      "Epoch 234/300\n",
      "88/88 [==============================] - 1s 7ms/step - loss: 2.3208e-07 - mse: 2.3208e-07 - val_loss: 1.0253e-05 - val_mse: 1.0253e-05\n",
      "Epoch 235/300\n",
      "88/88 [==============================] - 0s 4ms/step - loss: 5.2087e-09 - mse: 5.2087e-09 - val_loss: 1.0262e-05 - val_mse: 1.0262e-05\n",
      "Epoch 236/300\n",
      "88/88 [==============================] - 0s 4ms/step - loss: 3.0299e-09 - mse: 3.0299e-09 - val_loss: 1.0264e-05 - val_mse: 1.0264e-05\n",
      "Epoch 237/300\n",
      "88/88 [==============================] - 0s 4ms/step - loss: 2.9983e-07 - mse: 2.9983e-07 - val_loss: 1.0413e-05 - val_mse: 1.0413e-05\n",
      "Epoch 238/300\n",
      "88/88 [==============================] - 0s 4ms/step - loss: 3.5316e-08 - mse: 3.5316e-08 - val_loss: 1.0332e-05 - val_mse: 1.0332e-05\n",
      "Epoch 239/300\n",
      "88/88 [==============================] - 0s 3ms/step - loss: 2.3045e-09 - mse: 2.3045e-09 - val_loss: 1.0311e-05 - val_mse: 1.0311e-05\n",
      "Epoch 240/300\n",
      "88/88 [==============================] - 0s 4ms/step - loss: 3.2976e-08 - mse: 3.2976e-08 - val_loss: 1.0392e-05 - val_mse: 1.0392e-05\n",
      "Epoch 241/300\n",
      "88/88 [==============================] - 0s 4ms/step - loss: 2.8020e-07 - mse: 2.8020e-07 - val_loss: 1.0372e-05 - val_mse: 1.0372e-05\n",
      "Epoch 242/300\n",
      "88/88 [==============================] - 0s 4ms/step - loss: 8.0923e-09 - mse: 8.0923e-09 - val_loss: 1.0349e-05 - val_mse: 1.0349e-05\n",
      "Epoch 243/300\n",
      "88/88 [==============================] - 0s 4ms/step - loss: 1.3274e-07 - mse: 1.3274e-07 - val_loss: 1.0451e-05 - val_mse: 1.0451e-05\n",
      "Epoch 244/300\n",
      "88/88 [==============================] - 0s 4ms/step - loss: 3.3018e-08 - mse: 3.3018e-08 - val_loss: 1.0210e-05 - val_mse: 1.0210e-05\n",
      "Epoch 245/300\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 1.1450e-08 - mse: 1.1450e-08 - val_loss: 1.0194e-05 - val_mse: 1.0194e-05\n",
      "Epoch 246/300\n",
      "88/88 [==============================] - 0s 4ms/step - loss: 2.3743e-07 - mse: 2.3743e-07 - val_loss: 1.0276e-05 - val_mse: 1.0276e-05\n",
      "Epoch 247/300\n",
      "88/88 [==============================] - 0s 4ms/step - loss: 1.6343e-08 - mse: 1.6343e-08 - val_loss: 1.0233e-05 - val_mse: 1.0233e-05\n",
      "Epoch 248/300\n",
      "88/88 [==============================] - 1s 6ms/step - loss: 8.3422e-08 - mse: 8.3422e-08 - val_loss: 1.1180e-05 - val_mse: 1.1180e-05\n",
      "Epoch 249/300\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 6.1654e-08 - mse: 6.1654e-08 - val_loss: 1.0315e-05 - val_mse: 1.0315e-05\n",
      "Epoch 250/300\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 8.6017e-08 - mse: 8.6017e-08 - val_loss: 1.0375e-05 - val_mse: 1.0375e-05\n",
      "Epoch 251/300\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 1.9521e-07 - mse: 1.9521e-07 - val_loss: 1.0735e-05 - val_mse: 1.0735e-05\n",
      "Epoch 252/300\n",
      "88/88 [==============================] - 0s 4ms/step - loss: 5.7461e-08 - mse: 5.7461e-08 - val_loss: 1.0525e-05 - val_mse: 1.0525e-05\n",
      "Epoch 253/300\n",
      "88/88 [==============================] - 0s 4ms/step - loss: 4.9430e-08 - mse: 4.9430e-08 - val_loss: 1.0452e-05 - val_mse: 1.0452e-05\n",
      "Epoch 254/300\n",
      "88/88 [==============================] - 0s 4ms/step - loss: 1.3746e-07 - mse: 1.3746e-07 - val_loss: 1.0720e-05 - val_mse: 1.0720e-05\n",
      "Epoch 255/300\n",
      "88/88 [==============================] - 0s 6ms/step - loss: 1.2204e-07 - mse: 1.2204e-07 - val_loss: 1.0536e-05 - val_mse: 1.0536e-05\n",
      "Epoch 256/300\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 1.9690e-08 - mse: 1.9690e-08 - val_loss: 1.0497e-05 - val_mse: 1.0497e-05\n",
      "Epoch 257/300\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 4.0283e-08 - mse: 4.0283e-08 - val_loss: 1.0651e-05 - val_mse: 1.0651e-05\n",
      "Epoch 258/300\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 4.1152e-07 - mse: 4.1152e-07 - val_loss: 1.0683e-05 - val_mse: 1.0683e-05\n",
      "Epoch 259/300\n",
      "88/88 [==============================] - 0s 6ms/step - loss: 1.6331e-08 - mse: 1.6331e-08 - val_loss: 1.0690e-05 - val_mse: 1.0690e-05\n",
      "Epoch 260/300\n",
      "88/88 [==============================] - 1s 6ms/step - loss: 2.4515e-09 - mse: 2.4515e-09 - val_loss: 9.7328e-06 - val_mse: 9.7328e-06\n",
      "Epoch 261/300\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 1.1424e-07 - mse: 1.1424e-07 - val_loss: 4.5609e-06 - val_mse: 4.5609e-06\n",
      "Epoch 262/300\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 3.7251e-08 - mse: 3.7251e-08 - val_loss: 4.2517e-06 - val_mse: 4.2517e-06\n",
      "Epoch 263/300\n",
      "88/88 [==============================] - 1s 8ms/step - loss: 1.3639e-07 - mse: 1.3639e-07 - val_loss: 4.3472e-06 - val_mse: 4.3472e-06\n",
      "Epoch 264/300\n",
      "88/88 [==============================] - 0s 4ms/step - loss: 2.1353e-08 - mse: 2.1353e-08 - val_loss: 4.3158e-06 - val_mse: 4.3158e-06\n",
      "Epoch 265/300\n",
      "88/88 [==============================] - 0s 4ms/step - loss: 1.2784e-07 - mse: 1.2784e-07 - val_loss: 4.5673e-06 - val_mse: 4.5673e-06\n",
      "Epoch 266/300\n",
      "88/88 [==============================] - 1s 7ms/step - loss: 4.8451e-08 - mse: 4.8451e-08 - val_loss: 4.4248e-06 - val_mse: 4.4248e-06\n",
      "Epoch 267/300\n",
      "88/88 [==============================] - 1s 7ms/step - loss: 1.4214e-07 - mse: 1.4214e-07 - val_loss: 4.3271e-06 - val_mse: 4.3271e-06\n",
      "Epoch 268/300\n",
      "88/88 [==============================] - 1s 10ms/step - loss: 4.2633e-08 - mse: 4.2633e-08 - val_loss: 4.3230e-06 - val_mse: 4.3230e-06\n",
      "Epoch 269/300\n",
      "88/88 [==============================] - 1s 6ms/step - loss: 5.3143e-09 - mse: 5.3143e-09 - val_loss: 4.4036e-06 - val_mse: 4.4036e-06\n",
      "Epoch 270/300\n",
      "88/88 [==============================] - 1s 8ms/step - loss: 3.3750e-07 - mse: 3.3750e-07 - val_loss: 4.6019e-06 - val_mse: 4.6019e-06\n",
      "Epoch 271/300\n",
      "88/88 [==============================] - 1s 8ms/step - loss: 3.5736e-08 - mse: 3.5736e-08 - val_loss: 4.5279e-06 - val_mse: 4.5279e-06\n",
      "Epoch 272/300\n",
      "88/88 [==============================] - 1s 7ms/step - loss: 6.0606e-08 - mse: 6.0606e-08 - val_loss: 4.5441e-06 - val_mse: 4.5441e-06\n",
      "Epoch 273/300\n",
      "88/88 [==============================] - 1s 8ms/step - loss: 1.5362e-08 - mse: 1.5362e-08 - val_loss: 4.5198e-06 - val_mse: 4.5198e-06\n",
      "Epoch 274/300\n",
      "88/88 [==============================] - 1s 9ms/step - loss: 2.3778e-07 - mse: 2.3778e-07 - val_loss: 4.7875e-06 - val_mse: 4.7875e-06\n",
      "Epoch 275/300\n",
      "88/88 [==============================] - 1s 6ms/step - loss: 8.8725e-08 - mse: 8.8725e-08 - val_loss: 4.5996e-06 - val_mse: 4.5996e-06\n",
      "Epoch 276/300\n",
      "88/88 [==============================] - 0s 6ms/step - loss: 2.0757e-09 - mse: 2.0757e-09 - val_loss: 4.6300e-06 - val_mse: 4.6300e-06\n",
      "Epoch 277/300\n",
      "88/88 [==============================] - 0s 4ms/step - loss: 8.8858e-08 - mse: 8.8858e-08 - val_loss: 5.5809e-06 - val_mse: 5.5809e-06\n",
      "Epoch 278/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88/88 [==============================] - 0s 3ms/step - loss: 1.7313e-07 - mse: 1.7313e-07 - val_loss: 4.7521e-06 - val_mse: 4.7521e-06\n",
      "Epoch 279/300\n",
      "88/88 [==============================] - 0s 6ms/step - loss: 4.5101e-08 - mse: 4.5101e-08 - val_loss: 4.7137e-06 - val_mse: 4.7137e-06\n",
      "Epoch 280/300\n",
      "88/88 [==============================] - 1s 6ms/step - loss: 6.1731e-08 - mse: 6.1731e-08 - val_loss: 4.7642e-06 - val_mse: 4.7642e-06\n",
      "Epoch 281/300\n",
      "88/88 [==============================] - 0s 6ms/step - loss: 9.3938e-08 - mse: 9.3938e-08 - val_loss: 4.7792e-06 - val_mse: 4.7792e-06\n",
      "Epoch 282/300\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 3.1013e-08 - mse: 3.1013e-08 - val_loss: 4.7360e-06 - val_mse: 4.7360e-06\n",
      "Epoch 283/300\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 1.0878e-07 - mse: 1.0878e-07 - val_loss: 4.7896e-06 - val_mse: 4.7896e-06\n",
      "Epoch 284/300\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 1.2928e-07 - mse: 1.2928e-07 - val_loss: 4.8363e-06 - val_mse: 4.8363e-06\n",
      "Epoch 285/300\n",
      "88/88 [==============================] - 0s 4ms/step - loss: 1.4009e-08 - mse: 1.4009e-08 - val_loss: 5.0453e-06 - val_mse: 5.0453e-06\n",
      "Epoch 286/300\n",
      "88/88 [==============================] - 0s 4ms/step - loss: 2.7401e-07 - mse: 2.7401e-07 - val_loss: 4.8203e-06 - val_mse: 4.8203e-06\n",
      "Epoch 287/300\n",
      "88/88 [==============================] - 0s 4ms/step - loss: 5.4832e-09 - mse: 5.4832e-09 - val_loss: 4.8416e-06 - val_mse: 4.8416e-06\n",
      "Epoch 288/300\n",
      "88/88 [==============================] - 0s 4ms/step - loss: 1.7370e-09 - mse: 1.7370e-09 - val_loss: 4.8498e-06 - val_mse: 4.8498e-06\n",
      "Epoch 289/300\n",
      "88/88 [==============================] - 0s 4ms/step - loss: 1.7070e-07 - mse: 1.7070e-07 - val_loss: 4.9818e-06 - val_mse: 4.9818e-06\n",
      "Epoch 290/300\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 4.7631e-08 - mse: 4.7631e-08 - val_loss: 4.9040e-06 - val_mse: 4.9040e-06\n",
      "Epoch 291/300\n",
      "88/88 [==============================] - 0s 4ms/step - loss: 1.2477e-07 - mse: 1.2477e-07 - val_loss: 4.8979e-06 - val_mse: 4.8979e-06\n",
      "Epoch 292/300\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 2.4236e-08 - mse: 2.4236e-08 - val_loss: 4.9730e-06 - val_mse: 4.9730e-06\n",
      "Epoch 293/300\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 1.6761e-07 - mse: 1.6761e-07 - val_loss: 5.6813e-06 - val_mse: 5.6813e-06\n",
      "Epoch 294/300\n",
      "88/88 [==============================] - 0s 4ms/step - loss: 8.6214e-08 - mse: 8.6214e-08 - val_loss: 4.9958e-06 - val_mse: 4.9958e-06\n",
      "Epoch 295/300\n",
      "88/88 [==============================] - 0s 4ms/step - loss: 9.3907e-09 - mse: 9.3907e-09 - val_loss: 5.0115e-06 - val_mse: 5.0115e-06\n",
      "Epoch 296/300\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 6.5525e-08 - mse: 6.5525e-08 - val_loss: 5.0072e-06 - val_mse: 5.0072e-06\n",
      "Epoch 297/300\n",
      "88/88 [==============================] - 0s 6ms/step - loss: 2.1206e-07 - mse: 2.1206e-07 - val_loss: 5.3309e-06 - val_mse: 5.3309e-06\n",
      "Epoch 298/300\n",
      "88/88 [==============================] - 0s 4ms/step - loss: 1.7709e-07 - mse: 1.7709e-07 - val_loss: 5.1449e-06 - val_mse: 5.1449e-06\n",
      "Epoch 299/300\n",
      "88/88 [==============================] - 0s 4ms/step - loss: 5.2243e-09 - mse: 5.2243e-09 - val_loss: 5.1466e-06 - val_mse: 5.1466e-06\n",
      "Epoch 300/300\n",
      "88/88 [==============================] - 1s 6ms/step - loss: 1.0049e-07 - mse: 1.0049e-07 - val_loss: 5.2366e-06 - val_mse: 5.2366e-06\n"
     ]
    }
   ],
   "source": [
    "history=model.fit(X_train, y_train, epochs=300, batch_size=64, verbose=1, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tG0aDcxRhdol"
   },
   "source": [
    "### Plot the training error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 313
    },
    "executionInfo": {
     "elapsed": 1348,
     "status": "ok",
     "timestamp": 1669669129903,
     "user": {
      "displayName": "Mohamad Fares El Hajj Chehade",
      "userId": "11973934474938501082"
     },
     "user_tz": -120
    },
    "id": "wT_Wn-fogkr8",
    "outputId": "d57e4133-778e-4a23-eab0-41f20445638d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['loss', 'mse', 'val_loss', 'val_mse'])\n"
     ]
    }
   ],
   "source": [
    "print(history.history.keys()) # components that can be plotted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(history.history['loss']) # plot the training loss\n",
    "\n",
    "#plt.plot(history.history['val_loss']) # plot the validation loss \n",
    "\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "\n",
    "#plt.legend(['train', 'validation'], loc='upper left') # use legend if validation loss is also plotted\n",
    "\n",
    "plt.yscale('log') # log scale plot\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gDsLliL3prbO"
   },
   "source": [
    "### Evaluate the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_percentage_error as mape\n",
    "from sklearn.metrics import mean_squared_error as mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 894,
     "status": "ok",
     "timestamp": 1669669130784,
     "user": {
      "displayName": "Mohamad Fares El Hajj Chehade",
      "userId": "11973934474938501082"
     },
     "user_tz": -120
    },
    "id": "aCFknW9AptoD",
    "outputId": "3e75d3c4-e8af-44ac-be35-9fcbf12c242f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "94/94 [==============================] - 1s 4ms/step\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict(X_test)\n",
    "\n",
    "percentage_error = mape(y_pred,y_test)\n",
    "\n",
    "mean_squared_error = mse(y_pred,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the percentage error is:  0.09 %\n",
      "the mean squared error is:  1.850341204876662e-05\n",
      "the mean squared error of a dummy model is:  0.5\n"
     ]
    }
   ],
   "source": [
    "print(\"the percentage error is: \", round(percentage_error*100,2),\"%\")\n",
    "print(\"the mean squared error is: \", mean_squared_error, )\n",
    "print(\"the mean squared error of a dummy model is: \", 2*(0.5)**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OBYf2E_JtSlH"
   },
   "source": [
    "### Inverse Transform the predicted and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "executionInfo": {
     "elapsed": 17,
     "status": "ok",
     "timestamp": 1669669130785,
     "user": {
      "displayName": "Mohamad Fares El Hajj Chehade",
      "userId": "11973934474938501082"
     },
     "user_tz": -120
    },
    "id": "4ynsvzs7tVUj"
   },
   "outputs": [],
   "source": [
    "predicted = y_pred * (P_G_max - P_G_min)  + P_G_min\n",
    "actual = y_test * (P_G_max - P_G_min)  + P_G_min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def column_namer(name, n):\n",
    "    # returns an array of the names of columns of a dataframe\n",
    "    # name is the string that is repeated in all columns\n",
    "    # n is the number of such columns \n",
    "    \n",
    "    columns = []\n",
    "    \n",
    "    for i in range(n):\n",
    "        columns.append(name+str(i+1))\n",
    "    \n",
    "    return columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 270
    },
    "executionInfo": {
     "elapsed": 14,
     "status": "ok",
     "timestamp": 1669669130786,
     "user": {
      "displayName": "Mohamad Fares El Hajj Chehade",
      "userId": "11973934474938501082"
     },
     "user_tz": -120
    },
    "id": "Q7HUDVmLLWsG",
    "outputId": "0b8e4716-5dbe-4b5c-e5b3-19336cbbafa8"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PD1</th>\n",
       "      <th>PD2</th>\n",
       "      <th>PD3</th>\n",
       "      <th>PD4</th>\n",
       "      <th>PD5</th>\n",
       "      <th>PD6</th>\n",
       "      <th>PD7</th>\n",
       "      <th>PD8</th>\n",
       "      <th>PD9</th>\n",
       "      <th>PD10</th>\n",
       "      <th>...</th>\n",
       "      <th>PG2_test</th>\n",
       "      <th>PG2_pred</th>\n",
       "      <th>PG3_test</th>\n",
       "      <th>PG3_pred</th>\n",
       "      <th>PG4_test</th>\n",
       "      <th>PG4_pred</th>\n",
       "      <th>PG5_test</th>\n",
       "      <th>PG5_pred</th>\n",
       "      <th>PG6_test</th>\n",
       "      <th>PG6_pred</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20.373874</td>\n",
       "      <td>2.518507</td>\n",
       "      <td>7.562849</td>\n",
       "      <td>23.179659</td>\n",
       "      <td>28.563288</td>\n",
       "      <td>5.318394</td>\n",
       "      <td>10.747818</td>\n",
       "      <td>6.717240</td>\n",
       "      <td>8.151650</td>\n",
       "      <td>3.338627</td>\n",
       "      <td>...</td>\n",
       "      <td>57.874280</td>\n",
       "      <td>57.857361</td>\n",
       "      <td>22.204799</td>\n",
       "      <td>22.197926</td>\n",
       "      <td>31.510778</td>\n",
       "      <td>31.505520</td>\n",
       "      <td>15.511996</td>\n",
       "      <td>15.506646</td>\n",
       "      <td>15.511996</td>\n",
       "      <td>15.504433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>21.086838</td>\n",
       "      <td>2.429159</td>\n",
       "      <td>8.026110</td>\n",
       "      <td>22.342184</td>\n",
       "      <td>30.894099</td>\n",
       "      <td>5.698992</td>\n",
       "      <td>10.340713</td>\n",
       "      <td>6.348665</td>\n",
       "      <td>8.326624</td>\n",
       "      <td>3.529588</td>\n",
       "      <td>...</td>\n",
       "      <td>58.475681</td>\n",
       "      <td>58.450146</td>\n",
       "      <td>22.373191</td>\n",
       "      <td>22.364135</td>\n",
       "      <td>32.772711</td>\n",
       "      <td>32.757528</td>\n",
       "      <td>15.932976</td>\n",
       "      <td>15.923247</td>\n",
       "      <td>15.932976</td>\n",
       "      <td>15.920594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20.897990</td>\n",
       "      <td>2.394189</td>\n",
       "      <td>6.867199</td>\n",
       "      <td>23.401042</td>\n",
       "      <td>31.527076</td>\n",
       "      <td>5.840364</td>\n",
       "      <td>10.793929</td>\n",
       "      <td>6.341567</td>\n",
       "      <td>7.948685</td>\n",
       "      <td>3.458284</td>\n",
       "      <td>...</td>\n",
       "      <td>58.290188</td>\n",
       "      <td>58.264818</td>\n",
       "      <td>22.321253</td>\n",
       "      <td>22.314462</td>\n",
       "      <td>32.383488</td>\n",
       "      <td>32.368442</td>\n",
       "      <td>15.803132</td>\n",
       "      <td>15.792902</td>\n",
       "      <td>15.803132</td>\n",
       "      <td>15.790795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>23.406645</td>\n",
       "      <td>2.614595</td>\n",
       "      <td>7.405897</td>\n",
       "      <td>23.646149</td>\n",
       "      <td>32.947698</td>\n",
       "      <td>5.532945</td>\n",
       "      <td>10.348893</td>\n",
       "      <td>6.433159</td>\n",
       "      <td>8.252307</td>\n",
       "      <td>3.312791</td>\n",
       "      <td>...</td>\n",
       "      <td>59.060596</td>\n",
       "      <td>59.021244</td>\n",
       "      <td>22.536967</td>\n",
       "      <td>22.529265</td>\n",
       "      <td>34.000051</td>\n",
       "      <td>33.968854</td>\n",
       "      <td>16.342417</td>\n",
       "      <td>16.324394</td>\n",
       "      <td>16.342417</td>\n",
       "      <td>16.322206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20.533139</td>\n",
       "      <td>2.460550</td>\n",
       "      <td>7.897639</td>\n",
       "      <td>23.970808</td>\n",
       "      <td>29.658627</td>\n",
       "      <td>6.079893</td>\n",
       "      <td>11.737793</td>\n",
       "      <td>6.323484</td>\n",
       "      <td>7.663929</td>\n",
       "      <td>3.395631</td>\n",
       "      <td>...</td>\n",
       "      <td>58.161595</td>\n",
       "      <td>58.133240</td>\n",
       "      <td>22.285247</td>\n",
       "      <td>22.278923</td>\n",
       "      <td>32.113657</td>\n",
       "      <td>32.091938</td>\n",
       "      <td>15.713117</td>\n",
       "      <td>15.700368</td>\n",
       "      <td>15.713117</td>\n",
       "      <td>15.698603</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 32 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         PD1       PD2       PD3        PD4        PD5       PD6        PD7  \\\n",
       "0  20.373874  2.518507  7.562849  23.179659  28.563288  5.318394  10.747818   \n",
       "1  21.086838  2.429159  8.026110  22.342184  30.894099  5.698992  10.340713   \n",
       "2  20.897990  2.394189  6.867199  23.401042  31.527076  5.840364  10.793929   \n",
       "3  23.406645  2.614595  7.405897  23.646149  32.947698  5.532945  10.348893   \n",
       "4  20.533139  2.460550  7.897639  23.970808  29.658627  6.079893  11.737793   \n",
       "\n",
       "        PD8       PD9      PD10  ...   PG2_test   PG2_pred   PG3_test  \\\n",
       "0  6.717240  8.151650  3.338627  ...  57.874280  57.857361  22.204799   \n",
       "1  6.348665  8.326624  3.529588  ...  58.475681  58.450146  22.373191   \n",
       "2  6.341567  7.948685  3.458284  ...  58.290188  58.264818  22.321253   \n",
       "3  6.433159  8.252307  3.312791  ...  59.060596  59.021244  22.536967   \n",
       "4  6.323484  7.663929  3.395631  ...  58.161595  58.133240  22.285247   \n",
       "\n",
       "    PG3_pred   PG4_test   PG4_pred   PG5_test   PG5_pred   PG6_test   PG6_pred  \n",
       "0  22.197926  31.510778  31.505520  15.511996  15.506646  15.511996  15.504433  \n",
       "1  22.364135  32.772711  32.757528  15.932976  15.923247  15.932976  15.920594  \n",
       "2  22.314462  32.383488  32.368442  15.803132  15.792902  15.803132  15.790795  \n",
       "3  22.529265  34.000051  33.968854  16.342417  16.324394  16.342417  16.322206  \n",
       "4  22.278923  32.113657  32.091938  15.713117  15.700368  15.713117  15.698603  \n",
       "\n",
       "[5 rows x 32 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_matrix = st_x.inverse_transform(X_test)\n",
    "actual_target = actual \n",
    "predicted_target = predicted\n",
    "\n",
    "columns_PD = column_namer(\"PD\",n_loads)\n",
    "\n",
    "test_data = pd.DataFrame(input_matrix, columns=columns_PD)\n",
    "\n",
    "for i in range(n_gens):\n",
    "    test_data[\"PG\"+str(i+1)+\"_\"+\"test\"] = actual[:,i]\n",
    "    test_data[\"PG\"+str(i+1)+\"_\"+\"pred\"] = predicted[:,i]\n",
    "\n",
    "test_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(sum(predicted >= P_G_max)) + sum(sum(predicted <= P_G_min)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check how many line flow violations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_size = len(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fbus</th>\n",
       "      <th>tbus</th>\n",
       "      <th>r</th>\n",
       "      <th>x</th>\n",
       "      <th>b</th>\n",
       "      <th>rateA</th>\n",
       "      <th>rateB</th>\n",
       "      <th>rateC</th>\n",
       "      <th>ratio</th>\n",
       "      <th>angle</th>\n",
       "      <th>status</th>\n",
       "      <th>angmin</th>\n",
       "      <th>angmax</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.03</td>\n",
       "      <td>130</td>\n",
       "      <td>130</td>\n",
       "      <td>130</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-360</td>\n",
       "      <td>360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.02</td>\n",
       "      <td>130</td>\n",
       "      <td>130</td>\n",
       "      <td>130</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-360</td>\n",
       "      <td>360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.02</td>\n",
       "      <td>65</td>\n",
       "      <td>65</td>\n",
       "      <td>65</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-360</td>\n",
       "      <td>360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.00</td>\n",
       "      <td>130</td>\n",
       "      <td>130</td>\n",
       "      <td>130</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-360</td>\n",
       "      <td>360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.02</td>\n",
       "      <td>130</td>\n",
       "      <td>130</td>\n",
       "      <td>130</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-360</td>\n",
       "      <td>360</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   fbus  tbus     r     x     b  rateA  rateB  rateC  ratio  angle  status  \\\n",
       "0     0     1  0.02  0.06  0.03    130    130    130      0      0       1   \n",
       "1     0     2  0.05  0.19  0.02    130    130    130      0      0       1   \n",
       "2     1     3  0.06  0.17  0.02     65     65     65      0      0       1   \n",
       "3     2     3  0.01  0.04  0.00    130    130    130      0      0       1   \n",
       "4     1     4  0.05  0.20  0.02    130    130    130      0      0       1   \n",
       "\n",
       "   angmin  angmax  \n",
       "0    -360     360  \n",
       "1    -360     360  \n",
       "2    -360     360  \n",
       "3    -360     360  \n",
       "4    -360     360  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load the branch data \n",
    "\n",
    "branch = pd.read_excel(filename,sheet_name=\"Branch\")\n",
    "\n",
    "branch.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define line-related vectors \n",
    "\n",
    "P_flow = branch[\"rateA\"]\n",
    "reactance = branch[\"x\"]\n",
    "\n",
    "line_from = branch[\"fbus\"].tolist()\n",
    "line_to = branch[\"tbus\"].tolist()\n",
    "\n",
    "L = len(P_flow) # number of lines "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'G_buses' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-30-36ff75ebd1e6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mP_G\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtest_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mP_G\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mG_buses\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'G_buses' is not defined"
     ]
    }
   ],
   "source": [
    "P_G = np.zeros([test_size,n])\n",
    "P_G[G_buses].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load bus data\n",
    "\n",
    "bus = pd.read_excel(filename,sheet_name=\"Bus\")\n",
    "\n",
    "bus.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the power vector\n",
    "\n",
    "D_buses = bus[bus[\"Pd\"] != 0][\"bus\"]\n",
    "P_D = np.zeros([test_size,n])\n",
    "\n",
    "G_buses = gen[\"bus\"]\n",
    "P_G = np.zeros([test_size,n])\n",
    "\n",
    "for i in range(test_size):\n",
    "    P_D[i][D_buses] = input_matrix[i]\n",
    "    P_G[i][G_buses] = predicted[i] \n",
    "\n",
    "P = P_D - P_G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def theta_calculator(B_inv, P):\n",
    "    \n",
    "    return np.dot(B_inv, P)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_matrix(a):\n",
    "    \n",
    "    for line in a:\n",
    "        print ('  '.join(map(str, line)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "admittance = 1 / reactance\n",
    "\n",
    "# construct the bus-admittance matrix\n",
    "\n",
    "B = np.zeros([n,n])\n",
    "\n",
    "\n",
    "for line in range(L):\n",
    "    \n",
    "    i = line_from[line] \n",
    "    j = line_to[line] \n",
    "    \n",
    "    B[i][j] = - admittance[line]\n",
    "    B[j][i] = - admittance[line]\n",
    "\n",
    "    \n",
    "for node in nodes:\n",
    "    \n",
    "    B[node][node] = - (sum (B[node][j] for j in nodes))\n",
    "\n",
    "B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "B_prime = B\n",
    "\n",
    "for i in range(n):\n",
    "    B_prime[0][i] = 0\n",
    "    B_prime[i][0] = 0\n",
    "\n",
    "B_prime[0][0] = 1\n",
    "    \n",
    "B_inv = np.linalg.inv(B_prime) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert all vectors to per unit at 100 MVA base\n",
    "\n",
    "base = pd.read_excel(filename,sheet_name=\"Base\")\n",
    "Base = base[\"Base\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P_pu = P/Base\n",
    "P_pu[0] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta = np.zeros([test_size,n])\n",
    "\n",
    "for i in range(test_size):\n",
    "    theta[i] = theta_calculator(B_inv,P_pu[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# line flow constraints\n",
    "\n",
    "count_violations_per_sample = []\n",
    "count_violating_samples = 0\n",
    "\n",
    "for sample in range(test_size):\n",
    "    \n",
    "    theta_sample = theta[sample]\n",
    "    \n",
    "    violations_per_sample = 0\n",
    "\n",
    "    for line in range(L):\n",
    "\n",
    "        bound = reactance[line] * P_flow[line]\n",
    "        i = line_from[line] \n",
    "        j = line_to[line] \n",
    "\n",
    "        if (theta_sample[i] - theta_sample[j] >= bound or theta_sample[i] - theta_sample[j] <= - bound):\n",
    "            violatons_per_sample+=1\n",
    "    \n",
    "    count_violations_per_sample.append(violations_per_sample)\n",
    "    \n",
    "    if violations_per_sample != 0:\n",
    "        count_violating_samples += 1\n",
    "\n",
    "count_violating_samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MRp9DYiTb8jl"
   },
   "source": [
    "### Test on a new instance of the demand "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the default values\n",
    "\n",
    "default_load = pd.read_excel(filename, sheet_name = \"PD_Default\")\n",
    "P_D_default = default_load[\"PD_Default\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the margin of variation from default\n",
    "\n",
    "margin = pd.read_excel(filename,sheet_name = \"delta\")\n",
    "delta = margin[\"delta\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 14,
     "status": "ok",
     "timestamp": 1669669130787,
     "user": {
      "displayName": "Mohamad Fares El Hajj Chehade",
      "userId": "11973934474938501082"
     },
     "user_tz": -120
    },
    "id": "IHcsHU2yb-L1",
    "outputId": "aa6701b0-c4f9-40ec-d373-742df76d86e5"
   },
   "outputs": [],
   "source": [
    "P_D_new = []\n",
    "for i in range(n_loads):\n",
    "    \n",
    "    lower_bound = P_D_default[i] * (1 - delta)\n",
    "    upper_bound = P_D_default[i] * (1 + delta)\n",
    "    \n",
    "    random_demand = random.uniform(lower_bound,upper_bound)\n",
    "    \n",
    "    P_D_new.append(random_demand)\n",
    "\n",
    "\n",
    "x_new = st_x.transform([P_D_new])\n",
    "y_new = model.predict(x_new)\n",
    "\n",
    "y_new = y_new * (P_G_max - P_G_min) + P_G_min\n",
    "\n",
    "print(np.round(P_D_new,2))\n",
    "print(np.round(y_new,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen = pd.read_excel(filename,sheet_name=\"Gen\")\n",
    "G_buses = gen[\"bus\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''from project import project\n",
    "\n",
    "#import numpy as np\n",
    "\n",
    "N = len(X_test) # number of test samples\n",
    "\n",
    "y_pred_projected_intermediate = np.zeros([N,n])\n",
    "y_pred_projected = np.zeros([N,n_gens])\n",
    "\n",
    "for i in range(N):\n",
    "    y_pred_projected_intermediate[i] = project(input_matrix[i], predicted[i], n)\n",
    "    y_pred_projected[i] = y_pred_projected_intermediate[i][G_buses]'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_projected = y_pred_projected*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "percentage_error = mape(y_pred_projected,actual)\n",
    "\n",
    "mean_squared_error = mse(y_pred_projected,actual)\n",
    "\n",
    "print(\"the percentage error is: \", round(percentage_error*100,2),\"%\")\n",
    "print(\"the mean squared error is: \", mean_squared_error, )\n",
    "#print(\"the mean squared error of a dummy model is: \", 2*(0.5)**2)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyO+1kbWn3GVGHzJ6kkxF2cN",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
